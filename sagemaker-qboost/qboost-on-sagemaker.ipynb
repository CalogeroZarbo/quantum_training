{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Managed Quantum Machine Learning Example\n",
    "\n",
    "## Introduction\n",
    "In this example we will _sagemakerize_ the QBoost algorithm, which is a Quantum Binary Classifier published in 2008 by D-Wave & Google.\n",
    "Since you are reading this notebook, I assume you already followed the steps in the README.md files, but in any case I will report them here again:\n",
    "1. Run the script: create_wisc_datasets.py: this will create the folder _data_ in your local machine, and will generate inside it two files, one for training and one for testing\n",
    "2. Please get your AWS credentials, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY, and export them as environment variables\n",
    "3. Run the following commands to build the container and push the image in Amazon ECR:\n",
    "    chmod +wrx build_and_push.sh && ./build_and_push.sh qboost-sagemaker-example\n",
    "4. Once it's done, please open AWS SageMaker and start a Jupyter Notebook instance\n",
    "5. Follow the instruction in the next cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix where we will store everything we will produce following this tutorial\n",
    "prefix = 'DEMO-qboost-breast-cancer'\n",
    "\n",
    "# Define IAM role\n",
    "import boto3\n",
    "import re\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role() # Getting the Role, means getting the permissions connected with you AWS Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "from time import gmtime, strftime\n",
    "\n",
    "# The Session is essential in order to access the S3 Bucket without writing the whole URI.\n",
    "# It will also handle the creation of appropriate names for the S3 Buckets, EndPoint name, etc.\n",
    "sess = sage.Session() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now please create a folder in your Jupyter Lab environment and call it 'data'.\n",
    "# After you open it from the interface of Jupyter Lab, please and drag & drop from your computer\n",
    "# only the training csv previously created thanks to the script 'create_wisc_datasets.py' \n",
    "\n",
    "WORK_DIRECTORY = 'data' # The name of the folder with the training data just drag&dropped\n",
    "\n",
    "# With the following line of code you will upload the folder using the Session tool in a S3 Bucket. \n",
    "# The Session will take care about using the correct one\n",
    "data_location = sess.upload_data(WORK_DIRECTORY, key_prefix=prefix) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you would need to retrieve the account information, that are linked with this Sagemaker instance\n",
    "# In the SageMaker Jupyter Lab environment you can easily access them using the Session tool\n",
    "account = sess.boto_session.client('sts').get_caller_identity()['Account']\n",
    "region = sess.boto_session.region_name\n",
    "\n",
    "# This is the name of the image that you previously uploaded using the script:\n",
    "# './build_and_push.sh qboost-sagemaker-example'\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/qboost-sagemaker-example:latest'.format(account, region)\n",
    "\n",
    "# These are the variables that you need to train your QBoost on the D-Wave machine\n",
    "env = {\n",
    "    'DW_ENDPOINT': 'https://cloud.dwavesys.com/sapi',\n",
    "    'DW_TOKEN': 'Your DWave DEV Token',\n",
    "    'DW_SOLVER': 'DW_2000Q_2_1' , # Name of an available solver\n",
    "    'num_reads' : 1000, # Number of shots for the QPU\n",
    "    'tree_depth' : 2,\n",
    "}\n",
    "\n",
    "# Creating a SageMaker Estimator for your QBoost image\n",
    "qboost = sage.estimator.Estimator(image, # Docker Container image previously pushed\n",
    "                                  role, # Your account's role, you need to have the Sagemaker full priviledges\n",
    "                                  1, # Number of instances for your training. You need 1 since D-Wave is going to do all the work.\n",
    "                                  'ml.c4.2xlarge', # Instance type.\n",
    "                                  output_path=\"s3://{}/output\".format(sess.default_bucket()), # Where to put the trained model\n",
    "                                  sagemaker_session=sess, # Session tool, required\n",
    "                                  hyperparameters=env) # The parameters that we specified before for connected the D-Wave machine with Sagemaker\n",
    "\n",
    "qboost.fit(data_location) # Unleash the power of D-Wave.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to Deploy your model in order to be reachable and fully managed by SageMaker\n",
    "\n",
    "# I would like to highlight the fact that now you are not running the inference on QPU time, since\n",
    "# the power of the Quantum Optimisation has been delivered back to the classical world, ready to be deployed.\n",
    "\n",
    "from sagemaker.predictor import csv_serializer\n",
    "predictor = qboost.deploy(1, 'ml.m4.xlarge', serializer=csv_serializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we will upload the testing dataset, to use with the freshly created API\n",
    "\n",
    "# As we did before, create a folder in your Jupyter Lab environment and call it: 'data_testing'\n",
    "# Open it and drag & drop into it the file 'test_wisc_binary.csv' that was created before\n",
    "# and it should be in the 'data' folder in your local system (not the 'data' folder inside Sagemaker Jupyter Lab)\n",
    "\n",
    "shape=pd.read_csv(\"data_testing/test_wisc_binary.csv\", header=None) # Read the file\n",
    "\n",
    "import itertools\n",
    "\n",
    "# Select some random samples\n",
    "a = [50*i for i in range(3)] \n",
    "b = [40+i for i in range(10)]\n",
    "indices = [i+j for i,j in itertools.product(a,b)]\n",
    "\n",
    "# Create the test_data subset\n",
    "test_data=shape.iloc[indices[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the API with the test_data and get the predictions\n",
    "predictions = predictor.predict(test_data.values).decode('utf-8').split('\\n')[:-1]\n",
    "predictions = [float(x) for x in predictions] # Cast to float the predictions\n",
    "real_values = [float(x) for x in test_data.values[:,0]] # Cast to float the ground truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the accuracy of the trained model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(predictions, len(predictions))\n",
    "print(real_values, len(real_values))\n",
    "\n",
    "print('Accuracy', accuracy_score(predictions, real_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the endpoint in order to save money! $$ (to spend on QPU time ;) )\n",
    "sess.delete_endpoint(predictor.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We just saw how to create a Sagemaker Quantum Model, training it using D-Wave quantum machine directky from Sagemaker, and serve it in order to be used on production in a classical environment.\n",
    "The QPU time has been only used to _train_ the model, while the coocked recipe has been transported back in the classical machine, to perform inference through a normal API call.\n",
    "\n",
    "### Future Work\n",
    "In order to have our quantum model served in a proper production way you would need to:\n",
    "1. Create a Lambda function (which is serverless) on AWS that call the sagemaker model.\n",
    "2. Create an API Gateway toward the Lambda function that handle Error Codes, and Authentication\n",
    "\n",
    "I didn't explain these steps here since I will cover them later in a more complete example. However feel free to contact me on calogero.zarbo@docebo.com / calogero.zarbo@deeploans.ai / D-Wave Forum if you have any question, advice or feedback in general.\n",
    "\n",
    "### Why this work is relevant (IMHO)\n",
    "The main goal of this tutorial is not limited to implementation of QBoost algorithm, since it's a relatively old model (2008). My main goal was to provide to the community a complete example, that can be modified as much as anyone want and can be reused in their projects. In fact, you can substitute QBoost implementation with any other model and have it to run in a production ready way with very few steps.\n",
    "This example proves that the power of D-Wave quantum computer can be embedded in already existing framework, allowing the spread of this cutting-edge technology in multiple business cases.\n",
    "\n",
    "**_Once again, please feel free to contact me for anything_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
