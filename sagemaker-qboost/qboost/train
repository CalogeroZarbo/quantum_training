#!/usr/bin/env python3

# A sample training component that trains a simple scikit-learn decision tree model.
# This implementation works in File mode and makes no assumptions about the input file names.
# Input is specified as CSV with a data point in each row and the labels in the first column.

from __future__ import print_function

import os
import json
import pickle
import sys
import traceback

import pandas as pd

from sklearn import tree

# import necessary packages
from sklearn import preprocessing, metrics
from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier
from sklearn.datasets.mldata import fetch_mldata
from sklearn.datasets import load_breast_cancer
from dwave.system.samplers import DWaveSampler
from dwave.system.composites import EmbeddingComposite

from qboost import WeakClassifiers, QBoostClassifier, QboostPlus
import numpy as np

# These are the paths to where SageMaker mounts interesting things in your container.

prefix = '/opt/ml/'

input_path = prefix + 'input/data'
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
param_path = os.path.join(prefix, 'input/config/hyperparameters.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.
channel_name='training'
training_path = os.path.join(input_path, channel_name)

# Define the functions required in this example
def metric(y, y_pred):
    """
    :param y: true label
    :param y_pred: predicted label
    :return: metric score
    """

    return metrics.accuracy_score(y, y_pred)


def train():
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
        train_data = pd.concat(raw_data)

        # labels are in the first column
        y_train = np.array(train_data.ix[:,0])
        y_train = 2 * y_train - 1
        X_train = np.array(train_data.ix[:,1:])

        # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
        # strings, so we need to do any necessary conversions.
        lmd = trainingParams.get('lmd', 1.0)
        if lmd is not None:
            lmd = float(lmd)
        # define parameters used in this function
        NUM_READS = trainingParams.get('num_reads', 1000)
        if NUM_READS != 1000:
            NUM_READS = int(NUM_READS)
        NUM_WEAK_CLASSIFIERS = X_train.shape[1]#
        TREE_DEPTH = trainingParams.get('tree_depth', 2)
        if TREE_DEPTH != 2:
            TREE_DEPTH = int(TREE_DEPTH)

        DW_PARAMS = {'num_reads': NUM_READS,
                    'auto_scale': True,
                    'num_spin_reversal_transforms': 10,
                    'postprocess': 'optimization',
                    }


        DW_ENDPOINT = trainingParams.get('DW_ENDPOINT', 'https://cloud.dwavesys.com/sapi')
        DW_TOKEN = trainingParams.get('DW_TOKEN', None)
        DW_SLOVER = trainingParams.get('DW_SOLVER', 'DW_2000Q_2_1')

        if DW_ENDPOINT is None:
            raise Exception('You need to put your token in the Env Variable: DW_TOKEN')

        # define sampler
        dwave_sampler = DWaveSampler(endpoint=DW_ENDPOINT, token=DW_TOKEN, solver=DW_SLOVER)
        emb_sampler = EmbeddingComposite(dwave_sampler)

        N_train = len(X_train)
        
        print("\n======================================")
        print("Train size: %d" %(N_train))
        print('Num weak classifiers:', NUM_WEAK_CLASSIFIERS)

        # Preprocessing data
        scaler = preprocessing.StandardScaler()
        normalizer = preprocessing.Normalizer()

        X_train = scaler.fit_transform(X_train)
        X_train = normalizer.fit_transform(X_train)        

        # Qboost
        print('\nQBoost')
        clf = QBoostClassifier(n_estimators=NUM_WEAK_CLASSIFIERS, max_depth=TREE_DEPTH)
        clf.fit(X_train, y_train, emb_sampler, lmd=lmd, **DW_PARAMS)
        y_train = clf.predict(X_train)
        print(clf.estimator_weights)
        print('accu (train): %5.2f' % (metric(y_train, y_train)))

        # save the model
        with open(os.path.join(model_path, 'qboost-model.pkl'), 'wb') as out:
            pickle.dump(clf, out)
        print('Training complete.')

    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255) 


# The function to execute the training.
def train_old():
    print('Starting the training.')
    try:
        # Read in any hyperparameters that the user passed with the training job
        with open(param_path, 'r') as tc:
            trainingParams = json.load(tc)

        # Take the set of files and read them all into a single pandas dataframe
        input_files = [ os.path.join(training_path, file) for file in os.listdir(training_path) ]
        if len(input_files) == 0:
            raise ValueError(('There are no files in {}.\n' +
                              'This usually indicates that the channel ({}) was incorrectly specified,\n' +
                              'the data specification in S3 was incorrectly specified or the role specified\n' +
                              'does not have permission to access the data.').format(training_path, channel_name))
        raw_data = [ pd.read_csv(file, header=None) for file in input_files ]
        train_data = pd.concat(raw_data)

        # labels are in the first column
        train_y = train_data.ix[:,0]
        train_X = train_data.ix[:,1:]

        # Here we only support a single hyperparameter. Note that hyperparameters are always passed in as
        # strings, so we need to do any necessary conversions.
        max_leaf_nodes = trainingParams.get('max_leaf_nodes', None)
        if max_leaf_nodes is not None:
            max_leaf_nodes = int(max_leaf_nodes)

        # Now use scikit-learn's decision tree classifier to train the model.
        clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)
        clf = clf.fit(train_X, train_y)

        # save the model
        with open(os.path.join(model_path, 'decision-tree-model.pkl'), 'w') as out:
            pickle.dump(clf, out)
        print('Training complete.')
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)

if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
